{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63843b9-fe4a-4f59-9ea6-cca3f1cde204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8cfa6f9-dd4c-4705-baba-9ecba5156b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20424, 13)\n",
      "                                            personas additional_context  \\\n",
      "0  [i hate talking to people., i believe dragons ...     Social anxiety   \n",
      "1  [i have three daughters., my wife and i like t...                      \n",
      "2      [i hate the taste of fish., i like to paint.]                      \n",
      "3  [my favorite movie is good burger., i like can...                      \n",
      "4         [my hair is black., i like rock climbing.]      Rock climbing   \n",
      "\n",
      "                                  previous_utterance               context  \\\n",
      "0  [Wow, I am never shy. Do you have anxiety?, Ye...   wizard_of_wikipedia   \n",
      "1  [My turtle ran away from me today., Oh my god....  empathetic_dialogues   \n",
      "2  [Our son in the Army is taking a leave to visi...  empathetic_dialogues   \n",
      "3  [that's awesome , i like running in the mornin...               convai2   \n",
      "4  [Are there different skill levels? , I do not ...   wizard_of_wikipedia   \n",
      "\n",
      "                                       free_messages  \\\n",
      "0  [and why is that?, interesting but I know how ...   \n",
      "1  [that's funny. No. I let him roam around the h...   \n",
      "2  [Can't believe he grew up so quick, What do yo...   \n",
      "3  [Canning is great for storing food. Sometimes ...   \n",
      "4  [How would I start rock climbing?, I will do t...   \n",
      "\n",
      "                                     guided_messages  \\\n",
      "0  [I think it's because in my head, I think ever...   \n",
      "1  [What does your turtle eat?  Is it hard to tak...   \n",
      "2  [Yeah, kids grow up so quickly , We will proba...   \n",
      "3  [Wow, you've done a marathon?  I run a bit, bu...   \n",
      "4  [I would suggest a fitness place with a rock w...   \n",
      "\n",
      "                                         suggestions  \\\n",
      "0  {'convai2': ['i've no idea i am also very shy'...   \n",
      "1  {'convai2': ['no . i wanted to punch him more ...   \n",
      "2  {'convai2': ['i'm sure you and your husband ar...   \n",
      "3  {'convai2': ['yeah , that is a great food befo...   \n",
      "4  {'convai2': ['ha . well maybe you could start ...   \n",
      "\n",
      "          guided_chosen_suggestions  \\\n",
      "0         [wizard_of_wikipedia, , ]   \n",
      "1                      [, , , , , ]   \n",
      "2  [empathetic_dialogues, , , , , ]   \n",
      "3                      [, , , , , ]   \n",
      "4                      [, , , , , ]   \n",
      "\n",
      "                                    label_candidates        template_name  \\\n",
      "0  [[Oh nice! My brother in law is a lawyer and I...  guess-correct-order   \n",
      "1  [[I really can't imagine... does she do the ea...  guess-correct-order   \n",
      "2  [[It was an accident, actually. I'm a big fant...  guess-correct-order   \n",
      "3  [[well, what if Eminem plays video games?  not...  guess-correct-order   \n",
      "4  [[Haha yeah.  I'm not sure why people don't ex...  guess-correct-order   \n",
      "\n",
      "                                            template  \\\n",
      "0  Two people are having a conversation. Are the ...   \n",
      "1  Two people are having a conversation. Are the ...   \n",
      "2  Two people are having a conversation. Are the ...   \n",
      "3  Two people are having a conversation. Are the ...   \n",
      "4  Two people are having a conversation. Are the ...   \n",
      "\n",
      "                                      rendered_input rendered_output  \n",
      "0  Two people are having a conversation. Are the ...             No.  \n",
      "1  Two people are having a conversation. Are the ...            Yes.  \n",
      "2  Two people are having a conversation. Are the ...            Yes.  \n",
      "3  Two people are having a conversation. Are the ...            Yes.  \n",
      "4  Two people are having a conversation. Are the ...             No.  \n"
     ]
    }
   ],
   "source": [
    "# List of Parquet files\n",
    "files = [\n",
    "    \"data/test-00000-of-00001.parquet\",\n",
    "    \"data/train-00000-of-00001.parquet\",\n",
    "    \"data/validation-00000-of-00001.parquet\"\n",
    "]\n",
    "\n",
    "# Read all files and concatenate them into a single DataFrame\n",
    "df_combined = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "\n",
    "# Check the combined DataFrame\n",
    "print(df_combined.shape)  # To check number of rows and columns\n",
    "print(df_combined.head())  # Preview first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfa74b0-1a65-4eec-97a6-5ed7706e0542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20424, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72f9720-4fdd-4f2d-812e-10160da29935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20424 entries, 0 to 20423\n",
      "Data columns (total 13 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   personas                   20424 non-null  object\n",
      " 1   additional_context         20424 non-null  object\n",
      " 2   previous_utterance         20424 non-null  object\n",
      " 3   context                    20424 non-null  object\n",
      " 4   free_messages              20424 non-null  object\n",
      " 5   guided_messages            20424 non-null  object\n",
      " 6   suggestions                20424 non-null  object\n",
      " 7   guided_chosen_suggestions  20424 non-null  object\n",
      " 8   label_candidates           20424 non-null  object\n",
      " 9   template_name              20424 non-null  object\n",
      " 10  template                   20424 non-null  object\n",
      " 11  rendered_input             20424 non-null  object\n",
      " 12  rendered_output            20424 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0b94e9-645e-40e3-af5a-3be78021ee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (18381, 13), Validation set: (2043, 13)\n"
     ]
    }
   ],
   "source": [
    "# Proper train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, eval_df = train_test_split(df_combined, test_size=0.1, random_state=42)\n",
    "print(f\"Training set: {train_df.shape}, Validation set: {eval_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22309bdb-5588-4a3c-a70d-c7eb054b3c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Wow, I am never shy. Do you have anxiety?',\n",
       "       \"Yes. I end up sweating and blushing and feel like i'm going to throw up.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['previous_utterance'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22419500-d55e-432c-9e7d-9e913e36fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data in a way appropriate for conversational models\n",
    "def format_dialog(row):\n",
    "   \n",
    "    user_input = row[\"previous_utterance\"][0]  \n",
    "    bot_response = row[\"previous_utterance\"][1]  \n",
    "    \n",
    "    return {\n",
    "        \"text\": f\"<|endoftext|>User: {user_input}<|endoftext|>Bot: {bot_response}<|endoftext|>\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee587654-6ffb-4d7a-a487-03d8e5d5050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "train_data = [format_dialog(row) for _, row in train_df.iterrows()]\n",
    "eval_data = [format_dialog(row) for _, row in eval_df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1bc4fc9-de72-4044-a86a-651a3bc9e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7300ff9d-c952-47d5-ae8a-47b2e84c3fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2043\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14978d92-a2c0-4a24-8626-4682f00d14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization for efficiency\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "# Configure quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b02792a-8b46-4d78-bc31-b584abbe0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1343483c-20f3-4d73-a6ff-252daac711ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba8c64b-962f-478e-ac8c-7e4d78523c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8858d5d5-355a-4a22-9c99-946039d77062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  \n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28360d6f-721f-4b4a-99a0-14c5b713fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,081,344 || all params: 355,904,512 || trainable%: 0.3038\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "404940d5-3d28-4e2f-8d2f-aa0ba33fb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts\n",
    "    encodings = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n",
    "    \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f9921c5-e302-4a7e-b06a-8fd286ca56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18381/18381 [00:01<00:00, 14183.98 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2043/2043 [00:00<00:00, 13498.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3962e029-1d9e-4820-93d7-562a43f681a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2043\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21248fe0-bd97-46dd-b16b-dffb2ad6ddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tfenv\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure training - optimized for RTX 3050\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dialogpt_lora_rtx3050\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  \n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,  \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,  \n",
    "    save_total_limit=2,  \n",
    "    logging_steps=200,\n",
    "    learning_rate=1e-4, \n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    warmup_steps=200, \n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing=False,\n",
    "    # Memory optimization\n",
    "    dataloader_pin_memory=False,  \n",
    "    torch_compile=False,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47209b37-ea3a-4411-9ddf-b4596fe05dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65d8944a-2a55-4578-ac7c-5d1d9e859833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "D:\\anaconda\\envs\\tfenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3444' max='3444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3444/3444 2:03:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.887400</td>\n",
       "      <td>0.861915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.860800</td>\n",
       "      <td>0.838297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.853800</td>\n",
       "      <td>0.830105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tfenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\anaconda\\envs\\tfenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\anaconda\\envs\\tfenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3444, training_loss=1.026591715220098, metrics={'train_runtime': 7422.882, 'train_samples_per_second': 7.429, 'train_steps_per_second': 0.464, 'total_flos': 1.2844203220402176e+16, 'train_loss': 1.026591715220098, 'epoch': 2.9992383853769993})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e713f9d5-4f21-4f64-8fbb-d126db82e2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./final_dialogpt_lora\\\\tokenizer_config.json',\n",
       " './final_dialogpt_lora\\\\special_tokens_map.json',\n",
       " './final_dialogpt_lora\\\\vocab.json',\n",
       " './final_dialogpt_lora\\\\merges.txt',\n",
       " './final_dialogpt_lora\\\\added_tokens.json',\n",
       " './final_dialogpt_lora\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "model.save_pretrained(\"./final_dialogpt_lora\")\n",
    "tokenizer.save_pretrained(\"./final_dialogpt_lora\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c00691-3722-42cd-8fa7-f5686d0a5d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
